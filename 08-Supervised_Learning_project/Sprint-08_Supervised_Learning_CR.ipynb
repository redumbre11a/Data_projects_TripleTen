{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "The project is accepted! Thanks for taking the time to improve it! I left a couple of new comments below to clarify some thins, please check them out! And good luck on the next sprint!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review**\n",
    "\n",
    "Hi, my name is Dmitry and I will be reviewing your project.\n",
    "  \n",
    "You can find my comments in colored markdown cells:\n",
    "  \n",
    "<div class=\"alert alert-success\">\n",
    "  If everything is done successfully.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-warning\">\n",
    "  If I have some (optional) suggestions, or questions to think about, or general comments.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-danger\">\n",
    "  If a section requires some corrections. Work can't be accepted with red comments.\n",
    "</div>\n",
    "  \n",
    "Please don't remove my comments, as it will make further review iterations much harder for me.\n",
    "  \n",
    "Feel free to reply to my comments or ask questions using the following template:\n",
    "  \n",
    "<div class=\"alert alert-info\">\n",
    "  For your comments and questions.\n",
    "</div>\n",
    "  \n",
    "First of all, thank you for turning in the project! You did a great job overall! There are only a couple of small issues that need to be fixed before the project is accepted. Let me know if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Bank Churn Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents <a id='back'></a>\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Data Overview](#data_overview)\n",
    "    * [Initialization](#initialization)\n",
    "    * [Load Data](load_data)\n",
    "* [Prepare the Data](#prepare_data)\n",
    "    * [Check for Duplicates](#duplicates)\n",
    "    * [Check for Missing Values](#missing_values)\n",
    "    * [Converting Data Types](#data_types)\n",
    "* [Class Balance Examination](#class_balance)\n",
    "    * [Model without Accounting for Imbalance](#raw_model)\n",
    "    * [Fixing Class Imbalance](#fixing_class_imbalance)\n",
    "        * [Upsampling Random Forest](#upsampling_random_forest)\n",
    "        * [Downsampling Random Forest](#downsampling_random_forest)\n",
    "        * [Upsampling Decision Tree](#upsampling_decision_tree)\n",
    "        * [Downsampling Decision Tree](#downsampling_decision_tree)\n",
    "    * [Final Model](#final_model)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='introduction'></a>\n",
    "\n",
    "Beta Bank customers are leaving: little by little, chipping away every month. The bankers figured out it’s cheaper to save the existing customers rather than to attract new ones.\n",
    "\n",
    "We need to predict whether a customer will leave the bank soon. Using the provided data on clients’ past behavior and termination of contracts with the bank (`/datasets/Churn.csv`), build a model with the maximum possible F1 score. \n",
    "\n",
    "To pass the project, you need an F1 score of at least 0.59. Check the F1 for the test set.\n",
    "Additionally, measure the AUC-ROC metric and compare it with the F1.\n",
    "\n",
    "[Back to Contents](#back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview <a id='data_overview'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization <a id='initialization'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data <a id='load_data'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataframe file and storing it to churn_df\n",
    "churn_df = pd.read_csv('/datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data <a id='prepare_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Print the general/summary information about the DataFrame\n",
    "churn_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print a sample of the data\n",
    "display(churn_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The data was loaded and inspected!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Data <a id='fix_data'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
      "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
      "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# the list of column names in the table\n",
    "print(churn_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "churn_df= churn_df.rename(columns = {\n",
    "    'RowNumber':'row_number',\n",
    "    'CustomerId':'customer_id',\n",
    "    'Surname':'surname',\n",
    "    'CreditScore':'credit_score',\n",
    "    'Geography':'geography',\n",
    "    'Gender':'gender',\n",
    "    'Age':'age',\n",
    "    'Tenure':'tenure',\n",
    "    'Balance':'balance',\n",
    "    'NumOfProducts':'num_of_products',\n",
    "    'HasCrCard':'has_cr_card',\n",
    "    'IsActiveMember':'is_active_member',\n",
    "    'EstimatedSalary':'estimated_salary',\n",
    "    'Exited':'exited'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['row_number', 'customer_id', 'surname', 'credit_score', 'geography',\n",
      "       'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_cr_card',\n",
      "       'is_active_member', 'estimated_salary', 'exited'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_number  customer_id   surname  credit_score geography  gender  age  \\\n",
       "0           1     15634602  Hargrave           619    France  Female   42   \n",
       "1           2     15647311      Hill           608     Spain  Female   41   \n",
       "2           3     15619304      Onio           502    France  Female   42   \n",
       "3           4     15701354      Boni           699    France  Female   39   \n",
       "4           5     15737888  Mitchell           850     Spain  Female   43   \n",
       "\n",
       "   tenure    balance  num_of_products  has_cr_card  is_active_member  \\\n",
       "0     2.0       0.00                1            1                 1   \n",
       "1     1.0   83807.86                1            0                 1   \n",
       "2     8.0  159660.80                3            1                 0   \n",
       "3     1.0       0.00                2            0                 0   \n",
       "4     2.0  125510.82                1            1                 1   \n",
       "\n",
       "   estimated_salary  exited  \n",
       "0         101348.88       1  \n",
       "1         112542.58       0  \n",
       "2         113931.57       1  \n",
       "3          93826.63       0  \n",
       "4          79084.10       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking result: the list of column names\n",
    "print(churn_df.columns)\n",
    "\n",
    "display(churn_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting of the column names was changed to snake case for consistency. Data types for columns do not need to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Alright!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Duplicates <a id='duplicates'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "['France', 'Germany', 'Spain']\n"
     ]
    }
   ],
   "source": [
    "# checking for duplicated rows\n",
    "print(churn_df.duplicated().sum())\n",
    "print()\n",
    "\n",
    "# checking for duplicate row_number\n",
    "print(churn_df.duplicated(subset='row_number').sum())\n",
    "print()\n",
    "\n",
    "# checking for duplicate customer_id\n",
    "print(churn_df.duplicated(subset='customer_id').sum())\n",
    "print()\n",
    "\n",
    "# checking for implicit duplicates\n",
    "sorted_geography = sorted(churn_df['geography'].unique())\n",
    "print(sorted_geography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicate rows or duplicates within columns were found that would impact the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Good!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Missing Values <a id='missing_values'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_number            0\n",
      "customer_id           0\n",
      "surname               0\n",
      "credit_score          0\n",
      "geography             0\n",
      "gender                0\n",
      "age                   0\n",
      "tenure              909\n",
      "balance               0\n",
      "num_of_products       0\n",
      "has_cr_card           0\n",
      "is_active_member      0\n",
      "estimated_salary      0\n",
      "exited                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# calculating missing values\n",
    "print(churn_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% missing tenure rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.09"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating missing tenure row percentages\n",
    "print('% missing tenure rows:')\n",
    "churn_df['tenure'].isna().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>15589475</td>\n",
       "      <td>Azikiwe</td>\n",
       "      <td>591</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140469.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>15766205</td>\n",
       "      <td>Yin</td>\n",
       "      <td>550</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103391.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90878.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>15768193</td>\n",
       "      <td>Trevisani</td>\n",
       "      <td>585</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146050.97</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86424.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>15702298</td>\n",
       "      <td>Parkhill</td>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125561.97</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164040.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>15651280</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>742</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136857.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84509.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9944</th>\n",
       "      <td>9945</td>\n",
       "      <td>15703923</td>\n",
       "      <td>Cameron</td>\n",
       "      <td>744</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190409.34</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>138361.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>9957</td>\n",
       "      <td>15707861</td>\n",
       "      <td>Nucci</td>\n",
       "      <td>520</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85216.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117369.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9964</th>\n",
       "      <td>9965</td>\n",
       "      <td>15642785</td>\n",
       "      <td>Douglas</td>\n",
       "      <td>479</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117593.48</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113308.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>9986</td>\n",
       "      <td>15586914</td>\n",
       "      <td>Nepean</td>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_number  customer_id    surname  credit_score geography  gender  age  \\\n",
       "30            31     15589475    Azikiwe           591     Spain  Female   39   \n",
       "48            49     15766205        Yin           550   Germany    Male   38   \n",
       "51            52     15768193  Trevisani           585   Germany    Male   36   \n",
       "53            54     15702298   Parkhill           655   Germany    Male   41   \n",
       "60            61     15651280     Hunter           742   Germany    Male   35   \n",
       "...          ...          ...        ...           ...       ...     ...  ...   \n",
       "9944        9945     15703923    Cameron           744   Germany    Male   41   \n",
       "9956        9957     15707861      Nucci           520    France  Female   46   \n",
       "9964        9965     15642785    Douglas           479    France    Male   34   \n",
       "9985        9986     15586914     Nepean           659    France    Male   36   \n",
       "9999       10000     15628319     Walker           792    France  Female   28   \n",
       "\n",
       "      tenure    balance  num_of_products  has_cr_card  is_active_member  \\\n",
       "30       NaN       0.00                3            1                 0   \n",
       "48       NaN  103391.38                1            0                 1   \n",
       "51       NaN  146050.97                2            0                 0   \n",
       "53       NaN  125561.97                1            0                 0   \n",
       "60       NaN  136857.00                1            0                 0   \n",
       "...      ...        ...              ...          ...               ...   \n",
       "9944     NaN  190409.34                2            1                 1   \n",
       "9956     NaN   85216.61                1            1                 0   \n",
       "9964     NaN  117593.48                2            0                 0   \n",
       "9985     NaN  123841.49                2            1                 0   \n",
       "9999     NaN  130142.79                1            1                 0   \n",
       "\n",
       "      estimated_salary  exited  \n",
       "30           140469.38       1  \n",
       "48            90878.13       0  \n",
       "51            86424.57       0  \n",
       "53           164040.94       1  \n",
       "60            84509.57       0  \n",
       "...                ...     ...  \n",
       "9944         138361.48       0  \n",
       "9956         117369.52       1  \n",
       "9964         113308.29       0  \n",
       "9985          96833.00       0  \n",
       "9999          38190.78       0  \n",
       "\n",
       "[909 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find rows with missing values\n",
    "rows_with_missing_values = churn_df[churn_df.isnull().any(axis=1)]\n",
    "\n",
    "# Display the rows with missing values\n",
    "display(rows_with_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_number          0\n",
      "customer_id         0\n",
      "surname             0\n",
      "credit_score        0\n",
      "geography           0\n",
      "gender              0\n",
      "age                 0\n",
      "tenure              0\n",
      "balance             0\n",
      "num_of_products     0\n",
      "has_cr_card         0\n",
      "is_active_member    0\n",
      "estimated_salary    0\n",
      "exited              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing \"tenure\" data\n",
    "churn_df.dropna(subset=['tenure'], inplace=True)\n",
    "\n",
    "# Confirming no more missing values\n",
    "print(churn_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values were dropped to maintain numeric data type for creating a model. The only missing values in the dataframe are limited to only within the 'tenure' column. Due to the high number of variables affecting a customer's tenure and relatively small amount of missing values, accounting for about 9% of the tenure data, the missing values will also not be imputed to maintain the data integrity and will be left as NaN. This is acceptable since the missing tenure data will not be crucial for the analysis and is not believed to introduce bias. The missing values also seem to be missing at random with no particualr trend, also making it more acceptable to drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Missing values were dealt with reasonably! Nice explanation of your thought process!\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data Types <a id='data_types'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "      <th>geography_France</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9995</td>\n",
       "      <td>15719294</td>\n",
       "      <td>Wood</td>\n",
       "      <td>800</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9091 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_number  customer_id    surname  credit_score  age  tenure  \\\n",
       "0              1     15634602   Hargrave           619   42     2.0   \n",
       "1              2     15647311       Hill           608   41     1.0   \n",
       "2              3     15619304       Onio           502   42     8.0   \n",
       "3              4     15701354       Boni           699   39     1.0   \n",
       "4              5     15737888   Mitchell           850   43     2.0   \n",
       "...          ...          ...        ...           ...  ...     ...   \n",
       "9994        9995     15719294       Wood           800   29     2.0   \n",
       "9995        9996     15606229   Obijiaku           771   39     5.0   \n",
       "9996        9997     15569892  Johnstone           516   35    10.0   \n",
       "9997        9998     15584532        Liu           709   36     7.0   \n",
       "9998        9999     15682355  Sabbatini           772   42     3.0   \n",
       "\n",
       "        balance  num_of_products  has_cr_card  is_active_member  \\\n",
       "0          0.00                1            1                 1   \n",
       "1      83807.86                1            0                 1   \n",
       "2     159660.80                3            1                 0   \n",
       "3          0.00                2            0                 0   \n",
       "4     125510.82                1            1                 1   \n",
       "...         ...              ...          ...               ...   \n",
       "9994       0.00                2            0                 0   \n",
       "9995       0.00                2            1                 0   \n",
       "9996   57369.61                1            1                 1   \n",
       "9997       0.00                1            0                 1   \n",
       "9998   75075.31                2            1                 0   \n",
       "\n",
       "      estimated_salary  exited  geography_France  geography_Germany  \\\n",
       "0            101348.88       1                 1                  0   \n",
       "1            112542.58       0                 0                  0   \n",
       "2            113931.57       1                 1                  0   \n",
       "3             93826.63       0                 1                  0   \n",
       "4             79084.10       0                 0                  0   \n",
       "...                ...     ...               ...                ...   \n",
       "9994         167773.55       0                 1                  0   \n",
       "9995          96270.64       0                 1                  0   \n",
       "9996         101699.77       0                 1                  0   \n",
       "9997          42085.58       1                 1                  0   \n",
       "9998          92888.52       1                 0                  1   \n",
       "\n",
       "      geography_Spain  gender_Female  gender_Male  \n",
       "0                   0              1            0  \n",
       "1                   1              1            0  \n",
       "2                   0              1            0  \n",
       "3                   0              1            0  \n",
       "4                   1              1            0  \n",
       "...               ...            ...          ...  \n",
       "9994                0              1            0  \n",
       "9995                0              0            1  \n",
       "9996                0              0            1  \n",
       "9997                0              1            0  \n",
       "9998                0              0            1  \n",
       "\n",
       "[9091 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converting categorical variables, 'geography' and 'gender', to numerical form for use as input for a logistic regression model\n",
    "categorical_columns = ['geography', 'gender']\n",
    "\n",
    "# Apply one-hot encoding to these categorical columns\n",
    "churn_df_encoded = pd.get_dummies(churn_df, columns=categorical_columns, drop_first=False)\n",
    "\n",
    "display(churn_df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot-Encoding was used to convert the categorical variables that contained multiple categories into numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Categorical features were encoded successfully\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Balance Examination <a id='class_balance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7237\n",
      "1    1854\n",
      "Name: exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each class in the \"exited\" column\n",
    "class_counts = churn_df_encoded['exited'].value_counts()\n",
    "\n",
    "# Display the class counts\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above Series, it shows two rows: one for class 0 (customers who didn't exit) and one for class 1 (customers who exited). The results indicate that there is indeed a class imbalance, with class 0 being dominant since there are 7273 samples with class 0 and 1854 samples with class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Class imbalance was noted\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without Accounting for Imbalance <a id='raw_model'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8563049853372434\n",
      "Precision: 0.7894736842105263\n",
      "Recall: 0.4225352112676056\n",
      "F1 Score: 0.5504587155963302\n",
      "AUC-ROC Score: 0.8642703442879498\n",
      "Confusion Matrix:\n",
      "[[1048   32]\n",
      " [ 164  120]]\n",
      "Test Accuracy: 0.8629032258064516\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Select relevant numeric features (exclude non-numeric and non-predictive columns)\n",
    "numeric_features = ['credit_score', 'age', 'tenure', 'balance', 'num_of_products', 'has_cr_card', 'is_active_member', 'estimated_salary', 'geography_Germany', 'geography_Spain', 'gender_Male']\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "X_numeric = churn_df_encoded[numeric_features]\n",
    "\n",
    "# Separate features (X) and the target variable (y)\n",
    "X = X_numeric\n",
    "y = churn_df_encoded['exited']\n",
    "\n",
    "# Split your data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create and train a Random Forest classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Model Selection and Evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "precision = precision_score(y_val, y_pred_val)\n",
    "recall = recall_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val)\n",
    "roc_auc = roc_auc_score(y_val, best_model.predict_proba(X_val)[:, 1])\n",
    "confusion = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC-ROC Score: {roc_auc}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Finally, evaluate the model on the test set (unseen data)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment</b>\n",
    "\n",
    "1. The data was split into train and test sets. Later you split the same data into train and validation. Unfortunately that doesn't make much sense: we need to split the data into three parts from the start. The train set is then used to train the models, validation set is used to compare different models and tune their hyperparameters and the test set is used at the very end, to evaluate the final model for an unbiased estimate of its generalization performance. Although, as you're using cross-validation, it is possible to use just two sets: train and test, with train set used for cross-validation (you can compare different models using cross-validation as well as tune hyperpameteres), and the test set used to evaluate the final model.\n",
    "\n",
    "2. To calculate ROC-AUC, we need different inputs from the other metrics: instead of binary predictions (method `predict`) we need 'probabilities' (method `predict_proba`). The reason is that the ROC curve is constructed by varying the threshold of assigning positive class between 0 and 1, and for binary predictions the threshold is predefined\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Both problems were fixed!\n",
    "    \n",
    "Although, as you're using cross-validation, we don't really need to have both validation and test, we can just use cross-validation for hyperparameter tuning and model selection, and then evaluate the final model on the test set\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model, without accounting for the class imbalance, performed relatively well.\n",
    "\n",
    "The model achieved an accuracy of approximately 85.6% and precision of approximately 78.9%, meaning that when the model predicts a customer will churn, it is correct about 78.9% of the time.\n",
    "\n",
    "The recall of approximately 42.2% indicates that the model correctly identifies about 42.2% of all actual churn cases. Recall measures the proportion of actual positive cases that the model correctly predicted as positive.\n",
    "\n",
    "The F1 score of approximately 55.0% is the harmonic mean of precision and recall. It provides a balance between precision and recall, with higher values indicating a better balance.\n",
    "\n",
    "The AUC-ROC score of approximately 86.4% is a measure of the model's ability to distinguish between positive and negative cases. An AUC-ROC score of 0.5 indicates that a model's performance is similar to random guessing. Since the AUC-ROC score is above 0.5, it performs better than random guessing.\n",
    "\n",
    "The confusion matrix provides a breakdown of the model's predictions. In this case, the model made 120 true positives, 1048 true negatives, 32 false positives, and 164 false negatives..\n",
    "\n",
    "This trained model without accounting for the class imbalance was made for comparison purposes. Improvements in model performance will be necessary to meet the objectives of predicting churn with a higher F1 score (at least 0.59), as well as addressing the class imbalance, though it was not too far off in achieving the required score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  Makes sense for comparison purposes to use the same type of model for balanced and not balanced. instead of doing an additional two models later for random forest and decision tree without balancing, I decided to just show the unbalanced as a random forest model instead of logistic regression since I was pretty certain to begin with that I'd end up using the random forest model anyways\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Ok, great!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Alright, you trained a logistic regression model without taking the imbalance into account first. But it's difficult to see the effect of balancing because you're training different models in the next section. It would be nice if you tried those models without balancing as well, or if you tried applying balancing to the logistic regression model\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Class Imbalance <a id='fixing_class_imbalance'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling Random Forest Model <a id='upsampling_random_forest'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after Oversampling with Cross-Validation:\n",
      "Best Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "F1 Score on Validation Set: 0.5419354838709677\n",
      "AUC-ROC Score on Validation Set: 0.8439708345433856\n",
      "Results on Test Set:\n",
      "F1 Score on Test Set: 0.5790349417637272\n",
      "AUC-ROC Score on Test Set: 0.8488455427598525\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Separate features (X) and the target variable (y)\n",
    "X = X_numeric\n",
    "y = churn_df_encoded['exited']\n",
    "\n",
    "# Step 1: Split your data into train (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define a StratifiedKFold cross-validator with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 3: Hyperparameter Tuning (as previously described)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "best_f1_score = -1  # Initialize with a low value\n",
    "best_model = None\n",
    "\n",
    "for train_index, val_index in cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Step 4: Apply Oversampling within each fold\n",
    "    majority_indices = np.where(y_train_fold == 0)[0]\n",
    "    minority_indices = np.where(y_train_fold == 1)[0]\n",
    "    \n",
    "    # Calculate the oversampling ratio\n",
    "    oversampling_ratio = len(majority_indices) // len(minority_indices)\n",
    "    \n",
    "    # Randomly oversample the minority class within the fold\n",
    "    oversampled_minority_indices = np.random.choice(minority_indices, size=len(minority_indices) * oversampling_ratio, replace=True)\n",
    "    \n",
    "    # Combine the oversampled minority and majority class\n",
    "    oversampled_indices = np.concatenate((majority_indices, oversampled_minority_indices))\n",
    "    \n",
    "    # Create the oversampled training set for this fold\n",
    "    X_train_fold = X_train_fold.iloc[oversampled_indices]\n",
    "    y_train_fold = y_train_fold.iloc[oversampled_indices]\n",
    "    \n",
    "    rf_classifier.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_val_fold = rf_classifier.predict(X_val_fold)\n",
    "    f1_val_fold = f1_score(y_val_fold, y_pred_val_fold)\n",
    "    \n",
    "    if f1_val_fold > best_f1_score:\n",
    "        best_f1_score = f1_val_fold\n",
    "        best_model = rf_classifier\n",
    "\n",
    "# Step 5: Model Selection and Evaluation (including AUC-ROC) on the validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "\n",
    "# Calculate AUC-ROC on the validation set\n",
    "y_prob_val = best_model.predict_proba(X_val)[:, 1]\n",
    "roc_auc_val = roc_auc_score(y_val, y_prob_val)\n",
    "\n",
    "print(\"Results after Oversampling with Cross-Validation:\")\n",
    "print(f\"Best Parameters: {best_model.get_params()}\")\n",
    "print(f\"F1 Score on Validation Set: {f1_val}\")\n",
    "print(f\"AUC-ROC Score on Validation Set: {roc_auc_val}\")\n",
    "\n",
    "# Step 6: Evaluate the final model on the test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate AUC-ROC on the test set\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(\"Results on Test Set:\")\n",
    "print(f\"F1 Score on Test Set: {f1_test}\")\n",
    "print(f\"AUC-ROC Score on Test Set: {roc_auc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  I did not use imblearn because even though I am working through my project on the tripleten platform, it seems to not support the imblearn module as I would get this error \n",
    "    \n",
    "    ModuleNotFoundError                       Traceback (most recent call last)\n",
    "/tmp/ipykernel_27/1036495727.py in <module>\n",
    "      3 from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "      4 from sklearn.metrics import f1_score, roc_auc_score\n",
    "----> 5 from imblearn.over_sampling import RandomOverSampler\n",
    "      6 from imblearn.pipeline import Pipeline\n",
    "      7 from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ModuleNotFoundError: No module named 'imblearn'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Indeed, it is not installed by default, but you can install it like this:\n",
    "    \n",
    "```python\n",
    "!pip install --user imblearn \n",
    "```\n",
    "    \n",
    "Note that after installing it, the kernel needs to be reloaded before it can see the new library.\n",
    "    \n",
    "    \n",
    "In any case, it's cool that you tried to implement it on your own, as it allows you to understand how it all works under the hood.\n",
    "    \n",
    "There are a couple of issues though:\n",
    "    \n",
    "1. First of all, in k-fold cross-validation, we want to train k models of the same kind and average their scores, it doesn't make sense to report the best fold's score\n",
    "    \n",
    "2. And second, in your code in this cell, there is actually no hypeprarameter tuning: only the model with the default hyperparameters is trained. You'd need to add an outer loop wrapping the cross-validation to do that. Roughly it would look like this (this is pseudocode just for understanding):\n",
    "    \n",
    "```python\n",
    "for hyperparameter_values in hyperparameter_values_list:\n",
    "    f1_scores_folds = []\n",
    "    for train_index, val_index in cv.split(X_train, y_train):\n",
    "        ...\n",
    "        model = RandomForestClassifier(...) # you need to set hypeprarameters here in some way depending on how you store them\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        f1_val_fold = f1_score(y_val_fold, y_pred_val_fold)\n",
    "        f1_scores_folds.append(f1_val_fold)\n",
    "    f1_score_ = np.mean(f1_scores_folds)\n",
    "    if f1_score_ > best_f1_score:\n",
    "        best_f1_score = f1_score_\n",
    "        best_model = model\n",
    "    \n",
    "```\n",
    "    \n",
    "This is basically what GridSearchCV does under the hood.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "    \n",
    "It's cool that you used a grid search with cross-validation to tune hyperparameters, but it's not really appropriate to use oversampled data for cross-validation. The correct process is to apply oversampling in each fold separately, e.g. using [imblearn pipelines](https://imbalanced-learn.org/stable/references/generated/imblearn.pipeline.Pipeline.html#imblearn.pipeline.Pipeline) and [oversamplers](https://imbalanced-learn.org/stable/references/over_sampling.html)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the model's performance after applying oversampling with cross-validation resulted in an F1 score of approximately 0.5420 on the validation set and approximately 0.5790 on the test set. The AUC-ROC score was approximately 0.8440 on the validation set and approximately 0.8488 on the test set. These results for the oversampling approach did not significantly differ from the model's performance compared to the previous results without oversampling to balance the data, which had an F1 score of 0.55 and AUC-ROC score of 0.86. Additionally, the oversampling approach did not achieve the required F1 score of 0.59 on the validation set as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling Random Forest <a id='downsampling_random_forest'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after Downsampling with Cross-Validation:\n",
      "Best Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "F1 Score on Validation Set: 0.5991649269311066\n",
      "AUC-ROC Score on Validation Set: 0.8476247358433014\n",
      "Results on Test Set:\n",
      "F1 Score on Test Set: 0.5739320920043811\n",
      "AUC-ROC Score on Test Set: 0.8473609968261432\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Separate features (X) and the target variable (y)\n",
    "X = X_numeric\n",
    "y = churn_df_encoded['exited']\n",
    "\n",
    "# Step 1: Split your data into train (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define a StratifiedKFold cross-validator with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 3: Hyperparameter Tuning (as previously described)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "best_f1_score = -1  # Initialize with a low value\n",
    "best_model = None\n",
    "\n",
    "for train_index, val_index in cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Step 4: Apply Downsampling within each fold\n",
    "    majority_indices = np.where(y_train_fold == 0)[0]\n",
    "    minority_indices = np.where(y_train_fold == 1)[0]\n",
    "    \n",
    "    # Randomly undersample the majority class within the fold\n",
    "    undersampled_majority_indices = np.random.choice(majority_indices, size=len(minority_indices), replace=False)\n",
    "    \n",
    "    # Combine the undersampled majority and minority class\n",
    "    undersampled_indices = np.concatenate((undersampled_majority_indices, minority_indices))\n",
    "    \n",
    "    # Create the undersampled training set for this fold\n",
    "    X_train_fold = X_train_fold.iloc[undersampled_indices]\n",
    "    y_train_fold = y_train_fold.iloc[undersampled_indices]\n",
    "    \n",
    "    rf_classifier.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_val_fold = rf_classifier.predict(X_val_fold)\n",
    "    f1_val_fold = f1_score(y_val_fold, y_pred_val_fold)\n",
    "    \n",
    "    if f1_val_fold > best_f1_score:\n",
    "        best_f1_score = f1_val_fold\n",
    "        best_model = rf_classifier\n",
    "\n",
    "# Step 5: Model Selection and Evaluation (including AUC-ROC) on the validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "\n",
    "# Calculate AUC-ROC on the validation set\n",
    "y_prob_val = best_model.predict_proba(X_val)[:, 1]\n",
    "roc_auc_val = roc_auc_score(y_val, y_prob_val)\n",
    "\n",
    "print(\"Results after Downsampling with Cross-Validation:\")\n",
    "print(f\"Best Parameters: {best_model.get_params()}\")\n",
    "print(f\"F1 Score on Validation Set: {f1_val}\")\n",
    "print(f\"AUC-ROC Score on Validation Set: {roc_auc_val}\")\n",
    "\n",
    "# Step 6: Evaluate the final model on the test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate AUC-ROC on the test set\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(\"Results on Test Set:\")\n",
    "print(f\"F1 Score on Test Set: {f1_test}\")\n",
    "print(f\"AUC-ROC Score on Test Set: {roc_auc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Same as for upsampling, the correct way to apply downsampling in cross-validation is to downsample the train subset in each fold separately\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the model's performance after applying downsampling with cross-validation resulted in an F1 score of approximately 0.5992 on the validation set and approximately 0.5739 on the test set. The AUC-ROC score was approximately 0.8476 on the validation set and approximately 0.8474 on the test set.\n",
    "\n",
    "These results indicate that the downsampling approach with cross-validation has improved the model's F1 score on the validation set compared to the previous results that used an oversampling approach. However, the F1 score on the test set is slightly lower than the validation set. The AUC-ROC scores suggest good discrimination ability between classes, and the model performs well in terms of class separation. Additionally, this model with the downsampling approach was able to meet the required F1 score for the validation set of at least 0.59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling Decision Tree Model <a id='upsampling_decision_tree'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Upsampling with Decision Tree Classifier:\n",
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "F1 Score on Validation Set: 0.4885290148448043\n",
      "AUC-ROC Score on Validation Set: 0.677618748033973\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split your data into training and validation sets (adjust the test_size as needed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Identify indices for the majority (non-churn) and minority (churn) classes\n",
    "majority_indices = np.where(y_train == 0)[0]\n",
    "minority_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "# Step 2: Randomly oversample the minority class\n",
    "oversampled_minority_indices = np.random.choice(minority_indices, size=len(majority_indices), replace=True)\n",
    "\n",
    "# Combine the majority and oversampled minority class\n",
    "oversampled_indices = np.concatenate((majority_indices, oversampled_minority_indices))\n",
    "\n",
    "# Create the oversampled training set\n",
    "oversampled_X = X_train.iloc[oversampled_indices]\n",
    "oversampled_y = y_train.iloc[oversampled_indices]\n",
    "\n",
    "# Step 3: Hyperparameter Tuning for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(decision_tree_classifier, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(oversampled_X, oversampled_y)\n",
    "\n",
    "# Step 4: Model Selection and Evaluation (including AUC-ROC)\n",
    "best_model_upsampled = grid_search.best_estimator_\n",
    "y_pred_val_upsampled = best_model_upsampled.predict(X_val)\n",
    "f1_val_upsampled = f1_score(y_val, y_pred_val_upsampled)\n",
    "\n",
    "# Calculate AUC-ROC on the validation set\n",
    "y_prob_val_upsampled = best_model_upsampled.predict_proba(X_val)[:, 1]\n",
    "roc_auc_val_upsampled = roc_auc_score(y_val, y_prob_val_upsampled)\n",
    "\n",
    "print(\"Results for Upsampling with Decision Tree Classifier:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"F1 Score on Validation Set: {f1_val_upsampled}\")\n",
    "print(f\"AUC-ROC Score on Validation Set: {roc_auc_val_upsampled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is approximately 0.49 and the AUC-ROC score is approximately 0.68. Overall, the upsampling approach with a Decision Tree Classifier resulted in a moderate F1 score and a decent AUC-ROC score. While the F1 score could be improved, the model demonstrates the ability to distinguish between churn and non-churn customers to some extent. However, this model does not pass in achieving the required F1 score of 0.59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling Decision Tree Model <a id='downsampling_decision_tree'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Downsampling with Decision Tree Classifier:\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "F1 Score on Validation Set: 0.5137614678899083\n",
      "AUC-ROC Score on Validation Set: 0.7761671261773033\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split your data into training and validation sets (adjust the test_size as needed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Identify indices for the majority (non-churn) and minority (churn) classes\n",
    "majority_indices = np.where(y_train == 0)[0]\n",
    "minority_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "# Step 2: Randomly undersample the majority class\n",
    "undersampled_majority_indices = np.random.choice(majority_indices, size=len(minority_indices), replace=False)\n",
    "\n",
    "# Combine the undersampled majority and minority class\n",
    "undersampled_indices = np.concatenate((undersampled_majority_indices, minority_indices))\n",
    "\n",
    "# Create the undersampled training set\n",
    "undersampled_X = X_train.iloc[undersampled_indices]\n",
    "undersampled_y = y_train.iloc[undersampled_indices]\n",
    "\n",
    "# Step 3: Hyperparameter Tuning for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(decision_tree_classifier, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(undersampled_X, undersampled_y)\n",
    "\n",
    "# Step 4: Model Selection and Evaluation (including AUC-ROC)\n",
    "best_model_downsampled = grid_search.best_estimator_\n",
    "y_pred_val_downsampled = best_model_downsampled.predict(X_val)\n",
    "f1_val_downsampled = f1_score(y_val, y_pred_val_downsampled)\n",
    "\n",
    "# Calculate AUC-ROC on the validation set\n",
    "y_prob_val_downsampled = best_model_downsampled.predict_proba(X_val)[:, 1]\n",
    "roc_auc_val_downsampled = roc_auc_score(y_val, y_prob_val_downsampled)\n",
    "\n",
    "print(\"Results for Downsampling with Decision Tree Classifier:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"F1 Score on Validation Set: {f1_val_downsampled}\")\n",
    "print(f\"AUC-ROC Score on Validation Set: {roc_auc_val_downsampled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is approximately 0.51 and the AUC-ROC score is approximately 0.78. Overall, the downsampling approach with a Decision Tree Classifier was slightly improved compared to the previous upsampling approach. However, this model still does not pass in achieving the required F1 score of 0.59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Great, you successfully applied two different balancing methods and trained a couple of different models with upsampled/donwsampled data\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model <a id='final_model'></a> <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after Downsampling with Cross-Validation:\n",
      "Best Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "F1 Score on Validation Set: 0.5991649269311066\n",
      "AUC-ROC Score on Validation Set: 0.8476247358433014\n",
      "Results on Test Set:\n",
      "F1 Score on Test Set: 0.5739320920043811\n",
      "AUC-ROC Score on Test Set: 0.8473609968261432\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Separate features (X) and the target variable (y)\n",
    "X = X_numeric\n",
    "y = churn_df_encoded['exited']\n",
    "\n",
    "# Step 1: Split your data into train (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define a StratifiedKFold cross-validator with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 3: Hyperparameter Tuning (as previously described)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "best_f1_score = -1  # Initialize with a low value\n",
    "best_model = None\n",
    "\n",
    "for train_index, val_index in cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Step 4: Apply Downsampling within each fold\n",
    "    majority_indices = np.where(y_train_fold == 0)[0]\n",
    "    minority_indices = np.where(y_train_fold == 1)[0]\n",
    "    \n",
    "    # Randomly undersample the majority class within the fold\n",
    "    undersampled_majority_indices = np.random.choice(majority_indices, size=len(minority_indices), replace=False)\n",
    "    \n",
    "    # Combine the undersampled majority and minority class\n",
    "    undersampled_indices = np.concatenate((undersampled_majority_indices, minority_indices))\n",
    "    \n",
    "    # Create the undersampled training set for this fold\n",
    "    X_train_fold = X_train_fold.iloc[undersampled_indices]\n",
    "    y_train_fold = y_train_fold.iloc[undersampled_indices]\n",
    "    \n",
    "    rf_classifier.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_val_fold = rf_classifier.predict(X_val_fold)\n",
    "    f1_val_fold = f1_score(y_val_fold, y_pred_val_fold)\n",
    "    \n",
    "    if f1_val_fold > best_f1_score:\n",
    "        best_f1_score = f1_val_fold\n",
    "        best_model = rf_classifier\n",
    "\n",
    "# Step 5: Model Selection and Evaluation (including AUC-ROC) on the validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "\n",
    "# Calculate AUC-ROC on the validation set\n",
    "y_prob_val = best_model.predict_proba(X_val)[:, 1]\n",
    "roc_auc_val = roc_auc_score(y_val, y_prob_val)\n",
    "\n",
    "print(\"Results after Downsampling with Cross-Validation:\")\n",
    "print(f\"Best Parameters: {best_model.get_params()}\")\n",
    "print(f\"F1 Score on Validation Set: {f1_val}\")\n",
    "print(f\"AUC-ROC Score on Validation Set: {roc_auc_val}\")\n",
    "\n",
    "# Step 6: Evaluate the final model on the test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate AUC-ROC on the test set\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(\"Results on Test Set:\")\n",
    "print(f\"F1 Score on Test Set: {f1_test}\")\n",
    "print(f\"AUC-ROC Score on Test Set: {roc_auc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model chosen was the Random Forest model using the downsampling approach since it was able to achieve the required F1 score of at least 0.59. When a specific performance metric such as an F1 score is set as a requirement, you typically aim to achieve that performance on the validation set, not the test set. The F1 score on the validation set passed with a score of 0.599 with an AUC-ROC score is approximately 0.85 on the validation set. These results suggest that the model performs well in identifying churn and non-churn customers in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Awesome!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a id='conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset was found to have a class imbalance, favoring class 0 (customers who did not exit). For comparison purposes, a model was trained without taking into account the imbalance. The model actually performed relatively well, having a  F1 score of 0.55, indicating decent performance of a classification model and an AUC-ROC score of 0.86. An AUC-ROC score of 0.5 indicates random chance, while a higher score suggests better model performance, thus the model's performance was better than random guessing.\n",
    "\n",
    "Both upsampling and downsampling approaches were used to account for the class imbalance, trying both the Random Forest and Decision Tree model. The upsampling approach for the Random Forest model achieved a reasonably good F1 score of 0.54 and a strong AUC-ROC score of 0.84 on the validation set. However, it did not achieve the required F1 score to pass the project. The downsampling approach for the Random Forest model was able to achieve the passing F1 score of at least 0.59 and an AUC-ROC score of 0.85 on the validation set.\n",
    "\n",
    "The Decision Tree model was also tested for comparison purposes, but for both the upsampling and downsampling approach, while it had a moderate F1 score of approximately 0.5 for and decent AUC-ROC scores of approximately 0.7 for both approaches, it ultimately did not pass the required F1 score.\n",
    "\n",
    "Based on these results, the final model chosen was the downsampling approach for the Random Forest model since it achieved a passing F1 score of 0.59 and a high AUC-ROC score of 0.85. These results suggest that the model performs well in identifying churn and non-churn customers in the dataset and performs better than random guessing since the AUC-ROC score is higher than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Excellent summary!\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
